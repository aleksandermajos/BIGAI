{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "electric-tracy",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hapir\\anaconda3\\envs\\BIGAI\\lib\\site-packages\\torchaudio\\extension\\extension.py:13: UserWarning: torchaudio C++ extension is not available.\n",
      "  warnings.warn('torchaudio C++ extension is not available.')\n",
      "Using cache found in C:\\Users\\hapir/.cache\\torch\\hub\\snakers4_silero-models_master\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d781a6babfd4de3b1663aab56807de7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=117375227.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36c23ab3d7ae46a4be14f2f81609e490",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1036844.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "scientific kinds of solutions to it and often that's coming from ambitious rulers and longer jur ural er derand to glig to the low wanttogo are d modern of it yes more predictability better knowing the world around you the promise perhaps that this is something that will make things more profitable for the nation and so on that promiseed though is often think held out by the kinds people who hope that they might be tooffer the solutions so you might think it's the marin saying you know we have these trick y methods we have you know problems where we we get less and cut sort to find us selves again but that doesn't necessarily seem to the case the people really pushing for this to be on the agenda and for rewards to be givenwhere the people who might do well from them so people like mathematiciions potentially instrument makekers who could oftener those solions were off the ones most interested in this being something that would be\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hapir\\anaconda3\\envs\\BIGAI\\lib\\site-packages\\torch\\nn\\modules\\module.py:889: UserWarning: stft will soon require the return_complex parameter be given for real inputs, and will further require that return_complex=True in a future PyTorch release. (Triggered internally at  ..\\aten\\src\\ATen\\native\\SpectralOps.cpp:639.)\n",
      "  result = self.forward(*input, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import zipfile\n",
    "import torchaudio\n",
    "from glob import glob\n",
    "\n",
    "device = torch.device('cpu')  # gpu also works, but our models are fast enough for CPU\n",
    "\n",
    "model, decoder, utils = torch.hub.load(repo_or_dir='snakers4/silero-models',\n",
    "                                       model='silero_stt',\n",
    "                                       language='en', # also available 'de', 'es'\n",
    "                                       device=device)\n",
    "(read_batch, split_into_batches,\n",
    " read_audio, prepare_model_input) = utils  # see function signature for details\n",
    "\n",
    "# download a single file, any format compatible with TorchAudio (soundfile backend)\n",
    "torch.hub.download_url_to_file('https://opus-codec.org/static/examples/samples/speech_orig.wav',\n",
    "                               dst ='speech_orig.wav', progress=True)\n",
    "test_files = glob('english.wav')\n",
    "batches = split_into_batches(test_files, batch_size=10)\n",
    "input = prepare_model_input(read_batch(batches[0]),\n",
    "                            device=device)\n",
    "\n",
    "output = model(input)\n",
    "for example in output:\n",
    "    print(decoder(example.cpu()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lesser-affairs",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
