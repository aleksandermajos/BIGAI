{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e93ce463-ce40-4ec2-a48e-405fbd55d0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ae88e96-68be-4856-854d-7387eeb4fd17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://archive.ics.uci.edu/ml/machine-learning-databases/00331/sentiment%20labelled%20sentences.zip\n",
      "  40960/Unknown - 0s 4us/step"
     ]
    }
   ],
   "source": [
    "def download_and_read(url):\n",
    "    local_file = url.split('/')[-1]\n",
    "    local_file = local_file.replace(\"%20\", \" \")\n",
    "    p = tf.keras.utils.get_file(local_file, url,\n",
    "        extract=True, cache_dir=\".\")\n",
    "    local_folder = os.path.join(\"datasets\", local_file.split('.')[0])\n",
    "    labeled_sentences = []\n",
    "    for labeled_filename in os.listdir(local_folder):\n",
    "        if labeled_filename.endswith(\"_labelled.txt\"):\n",
    "            with open(os.path.join(\n",
    "                    local_folder, labeled_filename), \"r\") as f:\n",
    "                for line in f:\n",
    "                    sentence, label = line.strip().split('\\t')\n",
    "                    labeled_sentences.append((sentence, label))\n",
    "    return labeled_sentences\n",
    "labeled_sentences = download_and_read(      \n",
    "    \"https://archive.ics.uci.edu/ml/machine-learning-databases/\" + \n",
    "    \"00331/sentiment%20labelled%20sentences.zip\")\n",
    "sentences = [s for (s, l) in labeled_sentences]\n",
    "labels = [int(l) for (s, l) in labeled_sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0593642b-62b5-479a-ac7b-790180f8c22a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary size: 5271\n"
     ]
    }
   ],
   "source": [
    "# tokenize sentences\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts(sentences)\n",
    "vocab_size = len(tokenizer.word_counts)\n",
    "print(\"vocabulary size: {:d}\".format(vocab_size))\n",
    "\n",
    "word2idx = tokenizer.word_index\n",
    "idx2word = {v:k for (k, v) in word2idx.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "23bf6cf7-ea14-49cb-93f3-30155d0b5af9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(75, 16.0), (80, 18.0), (90, 22.0), (95, 26.0), (99, 36.0), (100, 71.0)]\n"
     ]
    }
   ],
   "source": [
    "seq_lengths = np.array([len(s.split()) for s in sentences])\n",
    "print([(p, np.percentile(seq_lengths, p)) for p\n",
    "    in [75, 80, 90, 95, 99, 100]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1206d55d-8dd9-4773-9eee-ed238812ac8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seqlen = 64\n",
    "# create dataset\n",
    "sentences_as_ints = tokenizer.texts_to_sequences(sentences)\n",
    "sentences_as_ints = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    sentences_as_ints, maxlen=max_seqlen)\n",
    "labels_as_ints = np.array(labels)\n",
    "dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (sentences_as_ints, labels_as_ints))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1e0fbb58-9cc3-417b-bcd7-771be53ecb75",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.shuffle(10000)\n",
    "test_size = len(sentences) // 3\n",
    "val_size = (len(sentences) - test_size) // 10\n",
    "test_dataset = dataset.take(test_size)\n",
    "val_dataset = dataset.skip(test_size).take(val_size)\n",
    "train_dataset = dataset.skip(test_size + val_size)\n",
    "batch_size = 64\n",
    "train_dataset = train_dataset.batch(batch_size)\n",
    "val_dataset = val_dataset.batch(batch_size)\n",
    "test_dataset = test_dataset.batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2b0573cc-6776-4c72-b7bc-eeccd72e0167",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentAnalysisModel(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, max_seqlen, **kwargs):\n",
    "        super(SentimentAnalysisModel, self).__init__(**kwargs)\n",
    "        self.embedding = tf.keras.layers.Embedding(\n",
    "            vocab_size, max_seqlen)\n",
    "        self.bilstm = tf.keras.layers.Bidirectional(\n",
    "            tf.keras.layers.LSTM(max_seqlen)\n",
    "        )\n",
    "        self.dense = tf.keras.layers.Dense(64, activation=\"relu\")\n",
    "        self.out = tf.keras.layers.Dense(1, activation=\"sigmoid\")\n",
    "    def call(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.bilstm(x)\n",
    "        x = self.dense(x)\n",
    "        x = self.out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4b5c1d0a-92cd-4d18-b385-e6bfaa86cfea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sentiment_analysis_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       multiple                  337408    \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  multiple                 66048     \n",
      " l)                                                              \n",
      "                                                                 \n",
      " dense (Dense)               multiple                  8256      \n",
      "                                                                 \n",
      " dense_1 (Dense)             multiple                  65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 411,777\n",
      "Trainable params: 411,777\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = SentimentAnalysisModel(vocab_size+1, max_seqlen)\n",
    "model.build(input_shape=(batch_size, max_seqlen))\n",
    "model.summary()\n",
    "# compile\n",
    "model.compile(\n",
    "    loss=\"binary_crossentropy\",\n",
    "    optimizer=\"adam\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4aac468d-8bee-49b3-9f63-00adbe7f5d5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "29/29 [==============================] - 7s 46ms/step - loss: 0.6910 - accuracy: 0.5417 - val_loss: 0.6866 - val_accuracy: 0.4950\n",
      "Epoch 2/10\n",
      "29/29 [==============================] - 0s 17ms/step - loss: 0.6231 - accuracy: 0.7022 - val_loss: 0.4042 - val_accuracy: 0.8550\n",
      "Epoch 3/10\n",
      "29/29 [==============================] - 0s 17ms/step - loss: 0.4163 - accuracy: 0.8317 - val_loss: 0.3112 - val_accuracy: 0.8950\n",
      "Epoch 4/10\n",
      "29/29 [==============================] - 1s 17ms/step - loss: 0.2362 - accuracy: 0.9161 - val_loss: 0.1851 - val_accuracy: 0.9450\n",
      "Epoch 5/10\n",
      "29/29 [==============================] - 1s 18ms/step - loss: 0.1730 - accuracy: 0.9433 - val_loss: 0.1284 - val_accuracy: 0.9450\n",
      "Epoch 6/10\n",
      "29/29 [==============================] - 0s 17ms/step - loss: 0.1113 - accuracy: 0.9644 - val_loss: 0.0386 - val_accuracy: 0.9950\n",
      "Epoch 7/10\n",
      "29/29 [==============================] - 1s 17ms/step - loss: 0.0672 - accuracy: 0.9811 - val_loss: 0.0536 - val_accuracy: 0.9850\n",
      "Epoch 8/10\n",
      "29/29 [==============================] - 0s 17ms/step - loss: 0.0558 - accuracy: 0.9856 - val_loss: 0.0265 - val_accuracy: 0.9900\n",
      "Epoch 9/10\n",
      "29/29 [==============================] - 0s 16ms/step - loss: 0.0551 - accuracy: 0.9856 - val_loss: 0.0433 - val_accuracy: 0.9800\n",
      "Epoch 10/10\n",
      "29/29 [==============================] - 0s 17ms/step - loss: 0.0370 - accuracy: 0.9900 - val_loss: 0.0485 - val_accuracy: 0.9900\n"
     ]
    }
   ],
   "source": [
    "data_dir = \"./data\"\n",
    "logs_dir = os.path.join(\"./logs\")\n",
    "best_model_file = os.path.join(data_dir, \"best_model.h5\")\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(best_model_file,\n",
    "    save_weights_only=True,\n",
    "    save_best_only=True)\n",
    "tensorboard = tf.keras.callbacks.TensorBoard(log_dir=logs_dir)\n",
    "num_epochs = 10\n",
    "history = model.fit(train_dataset, epochs=num_epochs,\n",
    "    validation_data=val_dataset,\n",
    "    callbacks=[checkpoint, tensorboard])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "887cf087-4eea-4707-93c3-ac5cc8840e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = SentimentAnalysisModel(vocab_size+1, max_seqlen)\n",
    "best_model.build(input_shape=(batch_size, max_seqlen))\n",
    "best_model.load_weights(best_model_file)\n",
    "best_model.compile(\n",
    "    loss=\"binary_crossentropy\",\n",
    "    optimizer=\"adam\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "03feb42f-8666-470c-b3a7-607dacbd87d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 1s 12ms/step - loss: 0.0267 - accuracy: 0.9940\n",
      "test loss: 0.027, test accuracy: 0.994\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = best_model.evaluate(test_dataset)\n",
    "print(\"test loss: {:.3f}, test accuracy: {:.3f}\".format(\n",
    "    test_loss, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "85dcd63e-cbd1-47da-b8f1-502cab4190e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\t0\tsoyo technology sucks\n",
      "0\t0\twill not be back\n",
      "1\t1\tis pretty funny\n",
      "1\t1\tblue ant is easy to use\n",
      "1\t1\ti can assure you that you won't be disappointed\n",
      "1\t1\tlovely little thriller from hitchcock with lots of nice shenanigans surrounding a murdered spy a kidnapped child a nasty church a foreign plot and some random taxidermists\n",
      "1\t1\tif there was ever a movie that needed word of mouth to promote this is it\n",
      "0\t0\ti felt asleep the first time i watched it so i can recommend it for insomniacs\n",
      "1\t1\tit's hard not to fall head over heels in love with that girl\n",
      "0\t0\tonly like 3 or 4 buildings used a couple of locations maybe poor hummh\n",
      "0\t0\tand the rest of it just sits there being awful with soldiers singing songs about the masculinity they pledge themselves to hairsplitting about purity the admiration of swords etc\n",
      "0\t0\tthe commercials are the most misleading\n",
      "0\t0\tit's too bad that everyone else involved didn't share crowe's level of dedication to quality for if they did we'd have a far better film on our hands than this sub par mess\n",
      "0\t0\tat no point in the proceedings does it look remotely like america\n",
      "0\t0\ti tried talking real loud but shouting on the telephone gets old and i was still told it wasn't great\n",
      "0\t0\tcumbersome design\n",
      "1\t1\tit's also great to see that renowned silent screenwriter frances marion hasn't missed a step going from silent to sound\n",
      "1\t1\tcheck it out\n",
      "0\t0\teew this location needs a complete overhaul\n",
      "1\t1\tso i bought about 10 of these and saved alot of money\n",
      "1\t1\tlove this headset\n",
      "0\t0\tso in a nutshell 1 the restaraunt smells like a combination of a dirty fish market and a sewer\n",
      "1\t1\treasonably priced also\n",
      "0\t0\twe waited for forty five minutes in vain\n",
      "1\t1\tif you want a sandwich just go to any firehouse\n",
      "0\t0\tbought mainly for the charger which broke soon after purchasing\n",
      "0\t0\tnow the burgers aren't as good the pizza which used to be amazing is doughy and flavorless\n",
      "1\t1\treally really good rice all the time\n",
      "1\t1\ta great way to finish a great\n",
      "0\t0\ti'll be looking for a new earpiece\n",
      "1\t1\tthe best electronics of the available fm transmitters\n",
      "1\t1\twe have gotten a lot of compliments on it\n",
      "1\t1\tgive it a try you will be happy you did\n",
      "1\t1\ti will be back many times soon\n",
      "1\t1\twe had a group of 70 when we claimed we would only have 40 and they handled us beautifully\n",
      "1\t1\tthe decor is nice and the piano music soundtrack is pleasant\n",
      "0\t0\tdo not waste your time\n",
      "1\t1\tit was also the right balance of war and love\n",
      "0\t0\ti guess i should have known that this place would suck because it is inside of the excalibur but i didn't use my common sense\n",
      "0\t0\toh yeah and the storyline was pathetic too\n",
      "0\t0\tpoor reliability\n",
      "1\t1\tthis movie is well balanced with comedy and drama and i thoroughly enjoyed myself\n",
      "1\t1\tthe color is even prettier than i thought it would be and the graphics are incredibly sharp\n",
      "1\t1\tfive star plus plus\n",
      "1\t1\tnice blanket of moz over top but i feel like this was done to cover up the subpar food\n",
      "1\t1\ti love that they put their food in nice plastic containers as opposed to cramming it in little paper takeout boxes\n",
      "1\t1\tthe folks at otto always make us feel so welcome and special\n",
      "0\t0\tmy brother in law who works at the mall ate here same day and guess what he was sick all night too\n",
      "1\t1\tyou can't beat the price on these\n",
      "0\t0\ti'm not really sure how joey's was voted best hot dog in the valley by readers of phoenix magazine\n",
      "1\t1\ttheir research and development division obviously knows what they're doing\n",
      "0\t0\tthis place is horrible and way overpriced\n",
      "1\t1\tthis is a witty and delightful adaptation of the dr seuss book brilliantly animated by upa's finest and thoroughly deserving of its academy award\n",
      "1\t1\tnice headphones for the price and they work great\n",
      "1\t1\thelen baxendale is a very credible lady macbeth who can be very cheerfull at times and sometimes she just looks like a naughty girl but deadly in her taste for blood and evil\n",
      "0\t0\tthe item received was counterfeit\n",
      "1\t1\tthe battery works great\n",
      "0\t0\tthe only thing i wasn't too crazy about was their guacamole as i don't like it purã©ed\n",
      "1\t1\tlinda cardellini is the only thing good in this film\n",
      "1\t1\tgreat place to have a couple drinks and watch any and all sporting events as the walls are covered with tv's\n",
      "1\t1\tthese were so good we ordered them twice\n",
      "1\t1\tjudith light is one of my favorite actresses and i think she does a superb job in this film\n",
      "0\t0\tputting the race card aside lets look at the major flaw in the film they destroy latifa's character\n",
      "1\t1\tthere was a warm feeling with the service and i felt like their guest for a special treat\n",
      "accuracy score: 0.987\n",
      "confusion matrix\n",
      "[[505   2]\n",
      " [ 11 482]]\n"
     ]
    }
   ],
   "source": [
    "labels, predictions = [], []\n",
    "idx2word[0] = \"PAD\"\n",
    "is_first_batch = True\n",
    "for test_batch in test_dataset:\n",
    "   inputs_b, labels_b = test_batch\n",
    "   pred_batch = best_model.predict(inputs_b)\n",
    "   predictions.extend([(1 if p > 0.5 else 0) for p in pred_batch])\n",
    "   labels.extend([l for l in labels_b])\n",
    "   if is_first_batch:\n",
    "       # print first batch of label, prediction, and sentence\n",
    "       for rid in range(inputs_b.shape[0]):\n",
    "           words = [idx2word[idx] for idx in inputs_b[rid].numpy()]\n",
    "           words = [w for w in words if w != \"PAD\"]\n",
    "           sentence = \" \".join(words)\n",
    "           print(\"{:d}\\t{:d}\\t{:s}\".format(\n",
    "               labels[rid], predictions[rid], sentence))\n",
    "       is_first_batch = False\n",
    "print(\"accuracy score: {:.3f}\".format(accuracy_score(labels, predictions)))\n",
    "print(\"confusion matrix\")\n",
    "print(confusion_matrix(labels, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70aa7649-e729-40e9-8547-8d861961855e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
