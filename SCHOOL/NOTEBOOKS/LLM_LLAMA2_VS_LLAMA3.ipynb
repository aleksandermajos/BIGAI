{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90fd58fd-8178-4a0e-bd4a-18543fe129b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama\n",
    "from pathlib import Path\n",
    "p = Path.cwd()\n",
    "path_beginning = str(p.home())+'/PycharmProjects/BIGAI/'\n",
    "path = path_beginning+\"MODELS/TEXT/\"\n",
    "llama2 = path+'llama-2-7b-chat.Q4_K_M.gguf'\n",
    "llama3 = path+'Meta-Llama-3-8B-Instruct.Q4_K_M.gguf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69072062-de00-4dc0-9c41-0a7c83f1dda3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 19 key-value pairs and 291 tensors from /home/bigai/PycharmProjects/BIGAI/MODELS/TEXT/llama-2-7b-chat.Q4_K_M.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  18:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V2\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 4096\n",
      "llm_load_print_meta: n_embd_v_gqa     = 4096\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 11008\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 6.74 B\n",
      "llm_load_print_meta: model size       = 3.80 GiB (4.84 BPW) \n",
      "llm_load_print_meta: general.name     = LLaMA v2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:   no\n",
      "ggml_cuda_init: CUDA_USE_TENSOR_CORES: yes\n",
      "ggml_cuda_init: found 2 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 4090, compute capability 8.9, VMM: yes\n",
      "  Device 1: NVIDIA GeForce RTX 4060 Ti, compute capability 8.9, VMM: yes\n",
      "llm_load_tensors: ggml ctx size =    0.33 MiB\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =    70.31 MiB\n",
      "llm_load_tensors:      CUDA0 buffer size =  2308.78 MiB\n",
      "llm_load_tensors:      CUDA1 buffer size =  1512.16 MiB\n",
      "...................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 1024\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:      CUDA0 KV buffer size =   320.00 MiB\n",
      "llama_kv_cache_init:      CUDA1 KV buffer size =   192.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  512.00 MiB, K (f16):  256.00 MiB, V (f16):  256.00 MiB\n",
      "llama_new_context_with_model:  CUDA_Host  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model: pipeline parallelism enabled (n_copies=4)\n",
      "llama_new_context_with_model:      CUDA0 compute buffer size =   136.01 MiB\n",
      "llama_new_context_with_model:      CUDA1 compute buffer size =   136.02 MiB\n",
      "llama_new_context_with_model:  CUDA_Host compute buffer size =    16.02 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 3\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.context_length': '4096', 'general.name': 'LLaMA v2', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '11008', 'llama.attention.layer_norm_rms_epsilon': '0.000001', 'llama.rope.dimension_count': '128', 'llama.attention.head_count': '32', 'tokenizer.ggml.bos_token_id': '1', 'llama.block_count': '32', 'llama.attention.head_count_kv': '32', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '15'}\n",
      "Using fallback chat format: None\n",
      "llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from /home/bigai/PycharmProjects/BIGAI/MODELS/TEXT/Meta-Llama-3-8B-Instruct.Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = hub\n",
      "llama_model_loader: - kv   2:                           llama.vocab_size u32              = 128256\n",
      "llama_model_loader: - kv   3:                       llama.context_length u32              = 8192\n",
      "llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   5:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  11:                       llama.rope.freq_base f32              = 500000.000000\n",
      "llama_model_loader: - kv  12:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.scores arr[f32,128256]  = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
      "llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000\n",
      "llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009\n",
      "llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% set loop_messages = messages %}{% ...\n",
      "llama_model_loader: - kv  21:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 256/128256 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 128256\n",
      "llm_load_print_meta: n_merges         = 280147\n",
      "llm_load_print_meta: n_ctx_train      = 8192\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 500000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 8192\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 7B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 8.03 B\n",
      "llm_load_print_meta: model size       = 4.58 GiB (4.89 BPW) \n",
      "llm_load_print_meta: general.name     = hub\n",
      "llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'\n",
      "llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'\n",
      "llm_load_print_meta: LF token         = 128 'Ä'\n",
      "llm_load_tensors: ggml ctx size =    0.33 MiB\n",
      "llm_load_tensors: offloading 32 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 33/33 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =   281.81 MiB\n",
      "llm_load_tensors:      CUDA0 buffer size =  2479.84 MiB\n",
      "llm_load_tensors:      CUDA1 buffer size =  1923.65 MiB\n",
      ".......................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 1024\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: freq_base  = 500000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:      CUDA0 KV buffer size =    80.00 MiB\n",
      "llama_kv_cache_init:      CUDA1 KV buffer size =    48.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  128.00 MiB, K (f16):   64.00 MiB, V (f16):   64.00 MiB\n",
      "llama_new_context_with_model:  CUDA_Host  output buffer size =     0.49 MiB\n",
      "llama_new_context_with_model: pipeline parallelism enabled (n_copies=4)\n",
      "llama_new_context_with_model:      CUDA0 compute buffer size =   136.01 MiB\n",
      "llama_new_context_with_model:      CUDA1 compute buffer size =   298.52 MiB\n",
      "llama_new_context_with_model:  CUDA_Host compute buffer size =    16.02 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 3\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'tokenizer.chat_template': \"{% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{{ '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\", 'tokenizer.ggml.eos_token_id': '128009', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'gpt2', 'general.architecture': 'llama', 'llama.rope.freq_base': '500000.000000', 'llama.context_length': '8192', 'general.name': 'hub', 'llama.vocab_size': '128256', 'general.file_type': '15', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '128000', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '8'}\n",
      "Using gguf chat template: {% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\n",
      "\n",
      "'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{{ '<|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "' }}\n",
      "Using chat eos_token: <|eot_id|>\n",
      "Using chat bos_token: <|begin_of_text|>\n"
     ]
    }
   ],
   "source": [
    "llama2 = Llama(model_path=llama2,n_gpu_layers=-1,n_ctx=1024,echo=False)\n",
    "llama3 = Llama(model_path=llama3,n_gpu_layers=-1,n_ctx=1024,echo=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8dcc24f5-6825-40d0-bf23-7a461edab541",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Apr 30 13:54:55 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.78                 Driver Version: 550.78         CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4090        Off |   00000000:01:00.0  On |                  Off |\n",
      "|  0%   49C    P2             66W /  450W |    6654MiB /  24564MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  NVIDIA GeForce RTX 4060 Ti     Off |   00000000:02:00.0 Off |                  N/A |\n",
      "|  0%   44C    P2             27W /  165W |    4248MiB /  16380MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A      1233      G   /usr/lib/xorg/Xorg                            299MiB |\n",
      "|    0   N/A  N/A      1898      G   /usr/bin/ksmserver                              7MiB |\n",
      "|    0   N/A  N/A      1900      G   /usr/bin/kded5                                  7MiB |\n",
      "|    0   N/A  N/A      1901      G   /usr/bin/kwin_x11                             127MiB |\n",
      "|    0   N/A  N/A      1948      G   /usr/bin/plasmashell                           47MiB |\n",
      "|    0   N/A  N/A      1998      G   ...c/polkit-kde-authentication-agent-1          7MiB |\n",
      "|    0   N/A  N/A      2270      G   ...86_64-linux-gnu/libexec/kdeconnectd          7MiB |\n",
      "|    0   N/A  N/A      2271      G   /usr/bin/latte-dock                            52MiB |\n",
      "|    0   N/A  N/A      2288      G   /usr/bin/kaccess                                7MiB |\n",
      "|    0   N/A  N/A      2359      G   ...-linux-gnu/libexec/DiscoverNotifier          7MiB |\n",
      "|    0   N/A  N/A      2613      G   ...-gnu/libexec/xdg-desktop-portal-kde          7MiB |\n",
      "|    0   N/A  N/A      2663      G   /usr/bin/kwalletd5                              7MiB |\n",
      "|    0   N/A  N/A     15911      G   /opt/google/chrome/chrome                       6MiB |\n",
      "|    0   N/A  N/A     15958      G   ...seed-version=20240429-180218.438000        114MiB |\n",
      "|    0   N/A  N/A     45068      G   ...erProcess --variations-seed-version         10MiB |\n",
      "|    0   N/A  N/A     72376      C   ...gai/anaconda3/envs/BIGAI/bin/python       5850MiB |\n",
      "|    1   N/A  N/A      1233      G   /usr/lib/xorg/Xorg                              4MiB |\n",
      "|    1   N/A  N/A     72376      C   ...gai/anaconda3/envs/BIGAI/bin/python       4234MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "def13498-290a-4af5-8ea3-5869cc3cc19d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_make_move(model: Llama, prompt: str) -> str:\n",
    "    \"\"\" Call a model with a prompt \"\"\"\n",
    "    res = model(prompt, stream=False, max_tokens=1024, temperature=0.8)\n",
    "    return res[\"choices\"][0][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "859aba93-be52-40c5-9ee4-200d4c556653",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "board = [[\"E\", \"E\", \"E\"],\n",
    "         [\"E\", \"E\", \"E\"],\n",
    "         [\"E\", \"E\", \"E\"]]\n",
    "\n",
    "def board_to_string(board_data: List) -> str:\n",
    "    \"\"\" Convert board to the string representation \"\"\"\n",
    "    return \"\\n\".join([\" \".join(x) for x in board_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "70c9823e-f289-4416-8a4b-6044ad1d282e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys_prompt1 = \"\"\"You play a tic-tac-toe game. You make a move by placing X, \n",
    "                 your opponent plays by placing O. Empty cells are marked \n",
    "                 with E. You can place X only to the empty cell.\"\"\"\n",
    "sys_prompt2 = \"\"\"You play a tic-tac-toe game. You make a move by placing O, \n",
    "                 your opponent plays by placing X. Empty cells are marked \n",
    "                 with E. You can place O only to the empty cell.\"\"\"\n",
    "game_prompt = \"\"\"What is your next move? Think in steps. \n",
    "                 Each row and column should be in range 1..3. Write\n",
    "                 the answer in JSON as {\"ROW\": ROW, \"COLUMN\": COLUMN}.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "364fdd6e-d039-45af-9cb3-9644a51200ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "template_llama2 = f\"\"\"<s>[INST]<<SYS>>{sys_prompt1}<</SYS>>\n",
    "Here is the board image:\n",
    "__BOARD__\\n\n",
    "{game_prompt}\n",
    "[/INST]\"\"\"\n",
    "\n",
    "template_llama3 = f\"\"\"<|begin_of_text|>\n",
    "<|start_header_id|>system<|end_header_id|>{sys_prompt2}<|eot_id|>\n",
    "<|start_header_id|>user<|end_header_id|>\n",
    "Here is the board image:\n",
    "__BOARD__\\n\n",
    "{game_prompt}\n",
    "<|eot_id|>\n",
    "<|start_header_id|>assistant<|end_header_id|>\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5fb834b9-73d5-417f-bd64-254ad45db140",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prompt_llama2(board: List) -> str:\n",
    "    \"\"\" Make Llama-2 prompt \"\"\"\n",
    "    return template_llama2.replace(\"__BOARD__\", board_to_string(board))\n",
    "\n",
    "\n",
    "def make_prompt_llama3(board: List) -> str:\n",
    "    \"\"\" Make Llama-3 prompt \"\"\"\n",
    "    return template_llama3.replace(\"__BOARD__\", board_to_string(board))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "338797ce-2c4d-419b-8339-2d574015fafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "def extract_json(response: str) -> Optional[dict]:\n",
    "    \"\"\" Extract dictionary from a response string \"\"\"\n",
    "    try:\n",
    "        # Models sometimes to a mistake, fix: {ROW: 1, COLUMN: 2} => {\"ROW\": 1, \"COLUMN\": 2}\n",
    "        response = response.replace('ROW:', '\"ROW\":').replace('COLUMN:', '\"COLUMN\":')\n",
    "        # Extract json from a response\n",
    "        pos_end = response.rfind(\"}\")\n",
    "        pos_start = response.rfind(\"{\")\n",
    "        return json.loads(response[pos_start:pos_end+1])\n",
    "    except Exception as exp:\n",
    "        print(f\"extract_json::cannot parse output: {exp}\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cac8a44b-2faa-413c-9004-666b2e205b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_move(board_data: List, move: Optional[dict], symb: str):\n",
    "    \"\"\" Update board with a new symbol \"\"\"\n",
    "    row, col = int(move[\"ROW\"]), int(move[\"COLUMN\"])\n",
    "    if 1 <= row <= 3 and 1 <= col <= 3:\n",
    "        if board_data[row - 1][col - 1] == \"E\":\n",
    "            board_data[row - 1][col - 1] = symb\n",
    "        else:\n",
    "            print(f\"Wrong move: cell {row}:{col} is not empty\")\n",
    "    else:\n",
    "        print(\"Wrong move: incorrect index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2cbe8456-fc00-4a53-bffa-effd62e2be07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_for_end_game(board_data: List) -> bool:\n",
    "    \"\"\" Check if there are no empty cells available \"\"\"\n",
    "    return board_to_string(board_data).find(\"E\") == -1\n",
    "\n",
    "def check_for_win(board_data: List) -> bool:\n",
    "    \"\"\" Check if the game is over \"\"\"\n",
    "    # Check Horizontal and Vertical lines\n",
    "    for ind in range(3):\n",
    "        if board_data[ind][0] == board_data[ind][1] == board_data[ind][2] and board_data[ind][0] != \"E\":\n",
    "            print(f\"{board_data[ind][0]} win!\")\n",
    "            return True\n",
    "        if board_data[0][ind] == board_data[1][ind] == board_data[2][ind] and board_data[0][ind] != \"E\":\n",
    "            print(f\"{board_data[0][ind]} win!\")\n",
    "            return True\n",
    "    # Check Diagonals\n",
    "    if board_data[0][0] == board_data[1][1] == board_data[2][2] and board_data[1][1] != \"E\" or \\\n",
    "       board_data[2][0] == board_data[1][1] == board_data[0][2] and board_data[1][1] != \"E\":\n",
    "        print(f\"{board_data[1][1]} win!\")\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "86a1f45a-9815-4233-bd9e-4c026fddfff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "num_wins1, num_wins2 = 0, 0\n",
    "times_1, times_2 = [], []\n",
    "\n",
    "\n",
    "def run_game():\n",
    "    \"\"\" Run a game between two models \"\"\"\n",
    "    board = [[\"E\", \"E\", \"E\"],\n",
    "             [\"E\", \"E\", \"E\"],\n",
    "             [\"E\", \"E\", \"E\"]]\n",
    "    \n",
    "    moves_limit = 20\n",
    "    for step in range(moves_limit):\n",
    "        print(f\"Step {step+1}\")\n",
    "    \n",
    "        # Move: Model-1\n",
    "        t_start = time.monotonic()\n",
    "        prompt = make_prompt_llama2(board)\n",
    "        result_str = llm_make_move(llama2, prompt)\n",
    "        times_1.append(time.monotonic() - t_start)\n",
    "    \n",
    "        new_data = extract_json(result_str)\n",
    "        if new_data is not None:\n",
    "            make_move(board, new_data, symb=\"X\")\n",
    "            if check_for_win(board):\n",
    "                print('**Model 1 Won**')\n",
    "                num_wins1 += 1\n",
    "                break\n",
    "            if check_for_end_game(board):\n",
    "                break\n",
    "    \n",
    "        # Move: Model-2\n",
    "        t_start = time.monotonic()\n",
    "        prompt = make_prompt_llama3(board)\n",
    "        result_str = llm_make_move(llama3, prompt)\n",
    "        times_2.append(time.monotonic() - t_start)\n",
    "    \n",
    "        new_data = extract_json(result_str)\n",
    "        if new_data is not None:\n",
    "            make_move(board, new_data, symb=\"O\")\n",
    "            if check_for_win(board):\n",
    "                print('**Model 2 Won**')\n",
    "                num_wins2 += 1\n",
    "                break\n",
    "            if check_for_end_game(board):\n",
    "                break\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "495b7dd9-2d2a-4a7a-90b7-e1709eb16906",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =     212.76 ms\n",
      "llama_print_timings:      sample time =      18.70 ms /    44 runs   (    0.43 ms per token,  2352.82 tokens per second)\n",
      "llama_print_timings: prompt eval time =      77.00 ms /    65 tokens (    1.18 ms per token,   844.19 tokens per second)\n",
      "llama_print_timings:        eval time =     501.05 ms /    43 runs   (   11.65 ms per token,    85.82 tokens per second)\n",
      "llama_print_timings:       total time =     678.64 ms /   108 tokens\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =      85.59 ms\n",
      "llama_print_timings:      sample time =      61.09 ms /    37 runs   (    1.65 ms per token,   605.63 tokens per second)\n",
      "llama_print_timings: prompt eval time =      70.24 ms /    61 tokens (    1.15 ms per token,   868.43 tokens per second)\n",
      "llama_print_timings:        eval time =     485.09 ms /    36 runs   (   13.47 ms per token,    74.21 tokens per second)\n",
      "llama_print_timings:       total time =     875.29 ms /    97 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     212.76 ms\n",
      "llama_print_timings:      sample time =      49.03 ms /   119 runs   (    0.41 ms per token,  2427.13 tokens per second)\n",
      "llama_print_timings: prompt eval time =      61.74 ms /    64 tokens (    0.96 ms per token,  1036.54 tokens per second)\n",
      "llama_print_timings:        eval time =    1370.22 ms /   118 runs   (   11.61 ms per token,    86.12 tokens per second)\n",
      "llama_print_timings:       total time =    1705.74 ms /   182 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrong move: cell 2:1 is not empty\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =      85.59 ms\n",
      "llama_print_timings:      sample time =     232.31 ms /   138 runs   (    1.68 ms per token,   594.03 tokens per second)\n",
      "llama_print_timings: prompt eval time =      70.27 ms /    57 tokens (    1.23 ms per token,   811.15 tokens per second)\n",
      "llama_print_timings:        eval time =    1837.15 ms /   137 runs   (   13.41 ms per token,    74.57 tokens per second)\n",
      "llama_print_timings:       total time =    3105.38 ms /   194 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrong move: cell 2:1 is not empty\n",
      "\n",
      "Step 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     212.76 ms\n",
      "llama_print_timings:      sample time =      49.55 ms /   118 runs   (    0.42 ms per token,  2381.67 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =    1367.85 ms /   118 runs   (   11.59 ms per token,    86.27 tokens per second)\n",
      "llama_print_timings:       total time =    1640.94 ms /   119 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extract_json::cannot parse output: Expecting value: line 1 column 1 (char 0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =      85.59 ms\n",
      "llama_print_timings:      sample time =     104.22 ms /    62 runs   (    1.68 ms per token,   594.88 tokens per second)\n",
      "llama_print_timings: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_print_timings:        eval time =     833.42 ms /    62 runs   (   13.44 ms per token,    74.39 tokens per second)\n",
      "llama_print_timings:       total time =    1372.92 ms /    63 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =     212.76 ms\n",
      "llama_print_timings:      sample time =      30.95 ms /    72 runs   (    0.43 ms per token,  2326.56 tokens per second)\n",
      "llama_print_timings: prompt eval time =      60.46 ms /    60 tokens (    1.01 ms per token,   992.41 tokens per second)\n",
      "llama_print_timings:        eval time =     823.26 ms /    71 runs   (   11.60 ms per token,    86.24 tokens per second)\n",
      "llama_print_timings:       total time =    1051.15 ms /   131 tokens\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrong move: cell 1:2 is not empty\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =      85.59 ms\n",
      "llama_print_timings:      sample time =     120.90 ms /    73 runs   (    1.66 ms per token,   603.80 tokens per second)\n",
      "llama_print_timings: prompt eval time =      70.08 ms /    56 tokens (    1.25 ms per token,   799.11 tokens per second)\n",
      "llama_print_timings:        eval time =     964.13 ms /    72 runs   (   13.39 ms per token,    74.68 tokens per second)\n",
      "llama_print_timings:       total time =    1666.07 ms /   128 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O win!\n",
      "**Model 2 Won**\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "cannot access local variable 'num_wins2' where it is not associated with a value",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mrun_game\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[21], line 44\u001b[0m, in \u001b[0;36mrun_game\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_for_win(board):\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m**Model 2 Won**\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 44\u001b[0m     \u001b[43mnum_wins2\u001b[49m \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_for_end_game(board):\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: cannot access local variable 'num_wins2' where it is not associated with a value"
     ]
    }
   ],
   "source": [
    "run_game()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0faa13f-bb66-45b6-8822-479c503da8c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
