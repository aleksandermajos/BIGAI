{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d370c3e-1259-4e2b-baea-883c451495c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import gensim.downloader as api\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1fe625e-8b3a-4b2d-9774-97fe5d0e0232",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_and_read(url):\n",
    "   local_file = url.split('/')[-1]\n",
    "   p = tf.keras.utils.get_file(local_file, url,\n",
    "       extract=True, cache_dir=\".\")\n",
    "   labels, texts = [], []\n",
    "   local_file = os.path.join(\"datasets\", \"SMSSpamCollection\")\n",
    "   with open(local_file, \"r\") as fin:\n",
    "       for line in fin:\n",
    "           label, text = line.strip().split('\\t')\n",
    "           labels.append(1 if label == \"spam\" else 0)\n",
    "           texts.append(text)\n",
    "   return texts, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "319ca5c6-e7b9-432e-a4c9-9256459e1d18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip\n",
      "  98304/Unknown - 0s 4us/step"
     ]
    }
   ],
   "source": [
    "DATASET_URL = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip\"\n",
    "texts, labels = download_and_read(DATASET_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17969d56-5e70-4a43-b97e-860420e96b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb9c28be-1419-45de-b366-4d9af1270ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_sequences = tokenizer.texts_to_sequences(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "53e8de1d-b8c4-4e23-ad60-dffbfea53482",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_sequences = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    text_sequences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ee62042-0a03-4c94-a651-cbb5042bbd09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0, ...,   58, 4437,  144],\n",
       "       [   0,    0,    0, ...,  473,    6, 1941],\n",
       "       [   0,    0,    0, ...,  661,  393, 2997],\n",
       "       ...,\n",
       "       [   0,    0,    0, ...,  106,  251, 9011],\n",
       "       [   0,    0,    0, ...,  200,   12,   47],\n",
       "       [   0,    0,    0, ...,    2,   61,  269]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0dae6581-80de-4709-8357-522af7a76d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_records = len(text_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "116bb1bf-195f-4d89-a4e0-66fd186256f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seqlen = len(text_sequences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3ae01c2c-abcb-432a-9dd5-fc777fe01b54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5574 sentences, max length: 189\n"
     ]
    }
   ],
   "source": [
    "print(\"{:d} sentences, max length: {:d}\".format(\n",
    "    num_records, max_seqlen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8106e22d-67a7-4116-87bf-3aec83d7c20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 2\n",
    "cat_labels = tf.keras.utils.to_categorical(\n",
    "    labels, num_classes=NUM_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "78da2f70-0715-4c65-9bd5-4684f8c1f428",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       ...,\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "013df7a7-a4b5-4c1c-86da-537398e3d434",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size: 9013\n"
     ]
    }
   ],
   "source": [
    "word2idx = tokenizer.word_index\n",
    "idx2word = {v:k for k, v in word2idx.items()}\n",
    "word2idx[\"PAD\"] = 0\n",
    "idx2word[0] = \"PAD\"\n",
    "vocab_size = len(word2idx)\n",
    "print(\"vocab size: {:d}\".format(vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f02a8634-635f-437e-8ed2-88701126d711",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (text_sequences, cat_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8e5b0f33-def7-4b43-b50a-6aeb73228b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.shuffle(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3175e1e8-66b5-41e8-8b53-9c4e16fdca6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = num_records // 4\n",
    "val_size = (num_records - test_size) // 10\n",
    "test_dataset = dataset.take(test_size)\n",
    "val_dataset = dataset.skip(test_size).take(val_size)\n",
    "train_dataset = dataset.skip(test_size + val_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "95eefec2-9ebf-4fd5-97bc-7d3c5ff6f95a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3763"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3bad9bed-d889-49de-8d0f-e6ae9bb594ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "val_dataset = val_dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
    "train_dataset = train_dataset.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b9e7efef-6e79-4e0f-8941-a1b25854f27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_embedding_matrix(sequences, word2idx, embedding_dim,\n",
    "       embedding_file):\n",
    "   if os.path.exists(embedding_file):\n",
    "       E = np.load(embedding_file)\n",
    "   else:\n",
    "       vocab_size = len(word2idx)\n",
    "       E = np.zeros((vocab_size, embedding_dim))\n",
    "       word_vectors = api.load(EMBEDDING_MODEL)\n",
    "       for word, idx in word2idx.items():\n",
    "           try:\n",
    "               E[idx] = word_vectors.word_vec(word)\n",
    "           except KeyError:   # word not in embedding\n",
    "               pass\n",
    "       np.save(embedding_file, E)\n",
    "   return E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3b9ae26e-87d7-43a2-80ec-4fc001389a64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding matrix: (9013, 300)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aleks\\AppData\\Local\\Temp\\ipykernel_8304\\3225739946.py:11: DeprecationWarning: Call to deprecated `word_vec` (Use get_vector instead).\n",
      "  E[idx] = word_vectors.word_vec(word)\n"
     ]
    }
   ],
   "source": [
    "EMBEDDING_DIM = 300\n",
    "DATA_DIR = \"\"\n",
    "EMBEDDING_NUMPY_FILE = os.path.join(DATA_DIR, \"E.npy\")\n",
    "EMBEDDING_MODEL = \"glove-wiki-gigaword-300\"\n",
    "E = build_embedding_matrix(text_sequences, word2idx, \n",
    "   EMBEDDING_DIM,\n",
    "   EMBEDDING_NUMPY_FILE)\n",
    "print(\"Embedding matrix:\", E.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4b60578e-4390-4059-985d-f20b8281fb40",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_mode = \"scratch\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bb8a74ab-c4f0-4286-82d1-1dca77444547",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpamClassifierModel(tf.keras.Model):\n",
    "   def __init__(self, vocab_sz, embed_sz, input_length,\n",
    "           num_filters, kernel_sz, output_sz,\n",
    "           run_mode, embedding_weights,\n",
    "           **kwargs):\n",
    "       super(SpamClassifierModel, self).__init__(**kwargs)\n",
    "       if run_mode == \"scratch\":\n",
    "           self.embedding = tf.keras.layers.Embedding(vocab_sz,\n",
    "               embed_sz,\n",
    "               input_length=input_length,\n",
    "               trainable=True)\n",
    "       elif run_mode == \"vectorizer\":\n",
    "           self.embedding = tf.keras.layers.Embedding(vocab_sz,\n",
    "               embed_sz,\n",
    "               input_length=input_length,\n",
    "               weights=[embedding_weights],\n",
    "               trainable=False)\n",
    "       else:\n",
    "           self.embedding = tf.keras.layers.Embedding(vocab_sz,\n",
    "               embed_sz,\n",
    "               input_length=input_length,\n",
    "               weights=[embedding_weights],\n",
    "               trainable=True)\n",
    "       self.conv = tf.keras.layers.Conv1D(filters=num_filters,\n",
    "           kernel_size=kernel_sz,\n",
    "           activation=\"relu\")\n",
    "       self.dropout = tf.keras.layers.SpatialDropout1D(0.2)\n",
    "       self.pool = tf.keras.layers.GlobalMaxPooling1D()\n",
    "       self.dense = tf.keras.layers.Dense(output_sz,\n",
    "           activation=\"softmax\")\n",
    "   def call(self, x):\n",
    "       x = self.embedding(x)\n",
    "       x = self.conv(x)\n",
    "       x = self.dropout(x)\n",
    "       x = self.pool(x)\n",
    "       x = self.dense(x)\n",
    "       return x\n",
    "# model definition\n",
    "conv_num_filters = 256\n",
    "conv_kernel_size = 3\n",
    "model = SpamClassifierModel(\n",
    "   vocab_size, EMBEDDING_DIM, max_seqlen,\n",
    "   conv_num_filters, conv_kernel_size, NUM_CLASSES,\n",
    "   run_mode, E)\n",
    "model.build(input_shape=(None, max_seqlen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "4b880d43-90ee-4606-87bd-e9584c4f1142",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38b31f7-409a-43fc-941b-92355a1429d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
