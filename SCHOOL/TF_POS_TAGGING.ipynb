{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "698be688-7df0-47ef-af8a-704a21838072",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package treebank to\n",
      "[nltk_data]     C:\\Users\\aleks\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package treebank is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"treebank\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6f1f42e-6a21-49ad-b445-36205fa465d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "301e8f1f-c3cb-4b79-8f59-ad95a3c49752",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of records: 3914\n"
     ]
    }
   ],
   "source": [
    "def download_and_read(dataset_dir, num_pairs=None):\n",
    "    sent_filename = os.path.join(dataset_dir, \"treebank-sents.txt\")\n",
    "    poss_filename = os.path.join(dataset_dir, \"treebank-poss.txt\")\n",
    "    if not(os.path.exists(sent_filename) and os.path.exists(poss_filename)):\n",
    "        import nltk   \n",
    "        if not os.path.exists(dataset_dir):\n",
    "            os.makedirs(dataset_dir)\n",
    "        fsents = open(sent_filename, \"w\")\n",
    "        fposs = open(poss_filename, \"w\")\n",
    "        sentences = nltk.corpus.treebank.tagged_sents()\n",
    "        for sent in sentences:\n",
    "            fsents.write(\" \".join([w for w, p in sent]) + \"\\n\")\n",
    "            fposs.write(\" \".join([p for w, p in sent]) + \"\\n\")\n",
    "        fsents.close()\n",
    "        fposs.close()\n",
    "    sents, poss = [], []\n",
    "    with open(sent_filename, \"r\") as fsent:\n",
    "        for idx, line in enumerate(fsent):\n",
    "            sents.append(line.strip())\n",
    "            if num_pairs is not None and idx >= num_pairs:\n",
    "                break\n",
    "    with open(poss_filename, \"r\") as fposs:\n",
    "        for idx, line in enumerate(fposs):\n",
    "            poss.append(line.strip())\n",
    "            if num_pairs is not None and idx >= num_pairs:\n",
    "                break\n",
    "    return sents, poss\n",
    "sents, poss = download_and_read(\"./datasets\")\n",
    "assert(len(sents) == len(poss))\n",
    "print(\"# of records: {:d}\".format(len(sents)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f5f8885a-3255-4055-976e-f1942a370b33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab sizes (source): 9001, (target): 39\n"
     ]
    }
   ],
   "source": [
    "def tokenize_and_build_vocab(texts, vocab_size=None, lower=True):\n",
    "    if vocab_size is None:\n",
    "        tokenizer = tf.keras.preprocessing.text.Tokenizer(lower=lower)\n",
    "    else:\n",
    "        tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "            num_words=vocab_size+1, oov_token=\"UNK\", lower=lower)\n",
    "    tokenizer.fit_on_texts(texts)\n",
    "    if vocab_size is not None:\n",
    "        # additional workaround, see issue 8092\n",
    "        # https://github.com/keras-team/keras/issues/8092\n",
    "        tokenizer.word_index = {e:i for e, i in\n",
    "            tokenizer.word_index.items() if \n",
    "            i <= vocab_size+1 }\n",
    "    word2idx = tokenizer.word_index\n",
    "    idx2word = {v:k for k, v in word2idx.items()}\n",
    "    return word2idx, idx2word, tokenizer\n",
    "word2idx_s, idx2word_s, tokenizer_s = tokenize_and_build_vocab(\n",
    "    sents, vocab_size=9000)\n",
    "word2idx_t, idx2word_t, tokenizer_t = tokenize_and_build_vocab(\n",
    "    poss, vocab_size=38, lower=False)\n",
    "source_vocab_size = len(word2idx_s)\n",
    "target_vocab_size = len(word2idx_t)\n",
    "print(\"vocab sizes (source): {:d}, (target): {:d}\".format(\n",
    "    source_vocab_size, target_vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "162377c3-1da7-4b03-80f8-9d28042c3bef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(75, 33.0), (80, 35.0), (90, 41.0), (95, 47.0), (99, 58.0), (100, 271.0)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(75, 33.0), (80, 35.0), (90, 41.0), (95, 47.0), (99, 58.0), (100, 271.0)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence_lengths = np.array([len(s.split()) for s in sents])\n",
    "print([(p, np.percentile(sequence_lengths, p))\n",
    "    for p in [75, 80, 90, 95, 99, 100]])\n",
    "[(75, 33.0), (80, 35.0), (90, 41.0), (95, 47.0), (99, 58.0), (100, 271.0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a54be46d-8726-41e0-a9ad-05d4442097f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seqlen = 271\n",
    "# convert sentences to sequence of integers\n",
    "sents_as_ints = tokenizer_s.texts_to_sequences(sents)\n",
    "sents_as_ints = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    sents_as_ints, maxlen=max_seqlen, padding=\"post\")\n",
    "# convert POS tags to sequence of (categorical) integers\n",
    "poss_as_ints = tokenizer_t.texts_to_sequences(poss)\n",
    "poss_as_ints = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    poss_as_ints, maxlen=max_seqlen, padding=\"post\")\n",
    "poss_as_catints = []\n",
    "for p in poss_as_ints:\n",
    "    poss_as_catints.append(tf.keras.utils.to_categorical(p,\n",
    "        num_classes=target_vocab_size+1, dtype=\"int32\"))\n",
    "poss_as_catints = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "    poss_as_catints, maxlen=max_seqlen)\n",
    "dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (sents_as_ints, poss_as_catints))\n",
    "idx2word_s[0], idx2word_t[0] = \"PAD\", \"PAD\"\n",
    "# split into training, validation, and test datasets\n",
    "dataset = dataset.shuffle(10000)\n",
    "test_size = len(sents) // 3\n",
    "val_size = (len(sents) - test_size) // 10\n",
    "test_dataset = dataset.take(test_size)\n",
    "val_dataset = dataset.skip(test_size).take(val_size)\n",
    "train_dataset = dataset.skip(test_size + val_size)\n",
    "# create batches\n",
    "batch_size = 128\n",
    "train_dataset = train_dataset.batch(batch_size)\n",
    "val_dataset = val_dataset.batch(batch_size)\n",
    "test_dataset = test_dataset.batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d167adf-2c4a-428e-954a-b9b5050ce58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_accuracy():\n",
    "    def masked_accuracy_fn(ytrue, ypred):\n",
    "        ytrue = tf.keras.backend.argmax(ytrue, axis=-1)\n",
    "        ypred = tf.keras.backend.argmax(ypred, axis=-1)\n",
    " \n",
    "        mask = tf.keras.backend.cast(\n",
    "            tf.keras.backend.not_equal(ypred, 0), tf.int32)\n",
    "        matches = tf.keras.backend.cast(\n",
    "            tf.keras.backend.equal(ytrue, ypred), tf.int32) * mask\n",
    "        numer = tf.keras.backend.sum(matches)\n",
    "        denom = tf.keras.backend.maximum(tf.keras.backend.sum(mask), 1)\n",
    "        accuracy =  numer / denom\n",
    "        return accuracy\n",
    "\n",
    "    return masked_accuracy_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9b34ebe7-4988-4fa1-99ab-c8b233526164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"pos_tagging_model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     multiple                  1152128   \n",
      "                                                                 \n",
      " spatial_dropout1d_1 (Spatia  multiple                 0         \n",
      " lDropout1D)                                                     \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirectio  multiple                 592896    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " time_distributed_1 (TimeDis  multiple                 20007     \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " activation_1 (Activation)   multiple                  0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,765,031\n",
      "Trainable params: 1,765,031\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "class POSTaggingModel(tf.keras.Model):\n",
    "    def __init__(self, source_vocab_size, target_vocab_size,\n",
    "            embedding_dim, max_seqlen, rnn_output_dim, **kwargs):\n",
    "        super(POSTaggingModel, self).__init__(**kwargs)\n",
    "        self.embed = tf.keras.layers.Embedding(\n",
    "            source_vocab_size, embedding_dim, input_length=max_seqlen)\n",
    "        self.dropout = tf.keras.layers.SpatialDropout1D(0.2)\n",
    "        self.rnn = tf.keras.layers.Bidirectional(\n",
    "            tf.keras.layers.GRU(rnn_output_dim, return_sequences=True))\n",
    "        self.dense = tf.keras.layers.TimeDistributed(\n",
    "            tf.keras.layers.Dense(target_vocab_size))\n",
    "        self.activation = tf.keras.layers.Activation(\"softmax\")\n",
    "    def call(self, x):\n",
    "        x = self.embed(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.rnn(x)\n",
    "        x = self.dense(x)\n",
    "        x = self.activation(x)\n",
    "        return x\n",
    "embedding_dim = 128\n",
    "rnn_output_dim = 256\n",
    "model = POSTaggingModel(source_vocab_size, target_vocab_size,\n",
    "    embedding_dim, max_seqlen, rnn_output_dim)\n",
    "model.build(input_shape=(batch_size, max_seqlen))\n",
    "model.summary()\n",
    "model.compile(\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    optimizer=\"adam\",\n",
    "    metrics=[\"accuracy\", masked_accuracy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ac0dc678-89e5-4b21-9ce7-c03f61df59af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_logs(data_dir):\n",
    "    logs_dir = os.path.join(data_dir, \"logs\")\n",
    "    shutil.rmtree(logs_dir, ignore_errors=True)\n",
    "    return logs_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4ac4fb98-922e-48a1-9970-d4803583f60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"./data\"\n",
    "logs_dir = clean_logs(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5bf464-5204-4d7e-a514-1288ab365eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 50\n",
    "best_model_file = os.path.join(data_dir, \"best_model.h5\")\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "    best_model_file,\n",
    "    save_weights_only=True,\n",
    "    save_best_only=True)\n",
    "tensorboard = tf.keras.callbacks.TensorBoard(log_dir=logs_dir)\n",
    "history = model.fit(train_dataset,\n",
    "    epochs=num_epochs,\n",
    "    validation_data=val_dataset,\n",
    "    callbacks=[checkpoint, tensorboard])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea03785-75ce-4937-bd76-da595d68eef3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
